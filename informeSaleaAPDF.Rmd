---
title: "TP1, Modelo Lineal"
author: "Gonzalo Barrera Borla y Octavio Martín Duarte "
date: "5 de Junio de 2019"
output:
  pdf_document: default
  header_includes:
   - \usepackage{float}
---

# Consignas

Escriba y entregue en un $script$ un programa de R que haga lo siguiente.

## a) Fije la Semilla

### i. Para $n=10$ genere $n$ datos $y_i$ que sigan el modelo lineal $y_i = 4+2 \cdot x_{i1} - 3 \cdot x_{i2} + 0,5 \cdot x_{i3} + {\varepsilon}_i$, $1 \leq i \leq n$ . Donde 

* $x_{1i} \sim \mathcal{U}(-5,5) ,iid.$
* $x_{2i} \sim \mathcal{U}(-5,5) ,iid.$
* $x_{3i} \sim \mathcal{U}(-5,5) ,iid.$
* $x_{4i} \sim \mathcal{U}(-5,5) ,iid.$
* $\varepsilon_{1i} \sim \mathcal{E}(\lambda=1/2)-2 ,iid.$

### ii. Ajuste el modelo $y_i = \beta_0+\beta_1 \cdot x_{i1} -\beta_{2} \cdot x_{i2} + \beta_{3} \cdot x_{i3} + \beta_{4} \cdot x_{i4} + u_i$

### iii. Guarde los parámetros estimados.

### iv. Construya el intervalo de confianza de nivel 0.90 para el parámetro $\beta_1$ y para el parámetro $\beta_4$ asumiendo normalidad de los errores. ¿Contienen estos intervalos a los verdaderos parámetros para la muestra simulada? Guarde en un nuevo objeto un uno si lo contiene, y un cero sino, para cada uno de los dos intervalos.
### v. Construya el intervalo de confianza de nivel asintótico 0.90 para el parámetro $\beta_1$ y para el parámetro $\beta_4$. ¿Contienen estos intervalos a los verdaderos parámetros para la muestra simulada? Guarde en un nuevo objeto un 1 si lo contiene, y un cero sino, para cada uno de los dos intervalos.


### vi. Repita los items a)i) hasta a)v) B = 1000 veces, de modo de tener una muestra de tamaño $B$ de los estimadores de cada $\beta_j$. ¿Diría que la distribución on de los estimadores de $\beta_2$ puede aproximarse por la normal? Haga gráficos que le permitan tomar esta decisión. ¿Qué proporción de los $B$ intervalos calculados para $\beta_1$ y $\beta_4$ basados en una muestra de $n$ observaciones contuvo al verdadero valor del parámetro? Responda para cada tipo de intervalo calculado.

## b) Repita $a$ para $n=25$ y $n=100$.


## c) Repita $a$ y $b$ para el caso de tener errores con distribución $Lognormal(\mu,\sigma^2)-e^{\mu+{\sigma^2}/2}$, tomando $\mu=0$ y $\sigma^2=1$. Si para alguna de las distribuciones no consigue convencerse de que los $\hat{\beta}$ tienen distribución que puede ser aproximada por una normal, repita para errores generados con esta distribución en el esquenma de simulación anterior pero con $n=250,500,1000,15000,2000,3000$. Exhiba los resultados en una tabla y comente brevemente sus conclusiones.

## d) Repita $c$ pero ahora con la distribución de errores $\mathcal{U}(-3;3)$ y con ${\chi^2}_k -k$ con $k=3$  y $t_k$ con $k=3$.

# Desarrollo 

## Método Empleado

  Dado que se solicita muchas repeticiones de consignas similares variando ciertos parámetros, comenzamos por elaborar una gran cantidad de simulaciones con todas las distribuciones contempladas para el error (y algunas de nuestra añadidura) con la máxima cantidad de repeticiones y el máximo número de muestras por simulación. Después vamos a ir acudiendo a ellas para tomar subconjuntos. Esto lo hicimos trivialmente y sin remuestrear, tomando los primeros $n$ elementos de cada simulación dado que como estas son efectivamente simulaciones de procesos aleatorios no nos pareció que el remuestreo fuera crítico a nuestros fines.

  Dejamos una serie de parámetros que permitirían generalizar aún más la simulación, el modelo del proceso generador de datos `beta_pgd`, el conjunto de `n` adoptados, la cantidad de simulaciones `n_sim`, etc.

  Dado que esta tabla y la siguiente involucran cantidades nada despreciables de cálculos, acá adjuntamos una versión del código similar a la usada pero parametrizada para muchas menos repeticiones, al lado aparecen comentados los verdaderos vectores que se usaron.
  
#### Parámetros

```{r preámbulos}

library('tidyverse')
library('stats')
library('future')
library('furrr')
library('knitr')
library('kableExtra')
set.seed(42)

# Coeficientes "platonicos" (i.e., del proceso generador de datos)
beta_pgd <- c(4, 2, -3, 0.5, 0)

metodos_intervalo <- c("asintotico", "exacto")
alfa <- 0.1

# Funciones generadoras de x_i
generadores_x <- list(
    "x1" = function(n) { runif(n, min=-5, max=5) },
    "x2" = function(n) { runif(n, min=-5, max=5) },
    "x3" = function(n) { runif(n, min=-5, max=5) },
    "x4" = function(n) { runif(n, min=-5, max=5) }
)

generadores_eps <- list(
  "normal" = function(n) { rnorm(n) },
  "exponencial" = function(n) { rexp(n, rate = 1/2) - 2 },
  "lognormal" = function(n) { exp(rnorm(n) - exp(0.5))  },
  "uniforme" = function(n) { runif(n, -3, 3) },
  "chi_cuadrado" = function(n) { rchisq(n, 3) - 3 },
  "student1" = function(n) { rt(n, 1) },
  "student3" = function(n) { rt(n, 3) }
)

funciones_a <- list(
  beta1 = c(0, 1, 0, 0, 0),
  beta4 = c(0, 0, 0, 0, 1)
)

generador_y <- function(x1, x2, x3, x4, beta_pgd, eps, ...)   {
  c(1, x1, x2, x3, x4) %*% beta_pgd + eps
}
```

#### Obtención de la muestra.

Para el vector con todos los valores de $n$, el conjunto de muestras pesa 3GB. Por esta razón, optamos por trabajar con otra tabla que acude a la tabla con las muestras cuando son necesarias y toma los datos pedidos para hallar los intervalos. 
Pusimos la salida de ambos tibble mostrando su forma, `muestras_maestras` es el conjunto de las muestras y `muestras_puntuales` conserva sólo la información necesaria para buscar en la tabla más grande. 

```{r muestra}
generar_muestra <- function(n, generadores_x, generador_eps, beta_pgd) {
  # Tibble vacio
  df <- tibble(.rows = n)
  # Genero variables regresoras y errores
  for (nombre in names(generadores_x)) {
    if (nombre != "y") {
      df[nombre] <- generadores_x[[nombre]](n)
    }
  df$eps <- generador_eps(n)
  }
  # Genero y
  df["y"] <- pmap_dbl(df, generador_y, beta_pgd=beta_pgd)

  return(df)
}

ayudante_generar_muestra <- function(distr_eps, generadores_x, beta_pgd, n) {
  generar_muestra(n,generadores_x, generadores_eps[[distr_eps]],beta_pgd=beta_pgd)
}


n_muestrales <- c(10, 25)
#n_muestrales <- c(10, 25, 100, 250, 500, 1000, 1500, 2000, 3000)

max_n_muestral <- max(n_muestrales)
n_sims <- 1000
muestras_maestras <- crossing(
  n_sim = seq(max_n_muestral),
  distr_eps = names(generadores_eps)) %>%
  mutate(
    muestra = future_map(.progress=TRUE,
                  distr_eps,
                  ayudante_generar_muestra,
                  generadores_x = generadores_x,
                  beta_pgd = beta_pgd,
                  n = max_n_muestral)
  )

muestras_maestras

#El '-3' es poco legible, buscar cómo sustraer una columna por nombre.

muestras_puntuales <- muestras_maestras[-3] %>%
  crossing(
    n = n_muestrales
  )

muestras_puntuales
```

## Intervalos


Para obtener los intervalos, usamos los parámetros que tiene guardada cada fila de `muestras_puntuales` y ejecutamos una función que lee en `muestras_maestras` de acuerdo a estos.

```{r obtención de los intervalos}
intervalo_conf <- function(a_vec, llamada_lm, alfa, metodo = "exacto") {

  betahat <- llamada_lm$coefficients
  # Matriz de covarianza estimada para los coeficientes
  Sigmahat <- vcov(llamada_lm)

  n_muestra <- nrow(llamada_lm$model)
  r <- llamada_lm$rank
  # Cualculo cuantil t o z, segun corresponda
  if (metodo == "exacto") {
    cuantil <- qt(p = 1 - alfa/2, df = n_muestra - r)
  } else if (metodo == "asintotico") {
    cuantil <- qnorm(p = 1 - alfa/2)
  } else {
    stop("Los unicos metodos soportados son 'exacto' y 'asintotico'")
  }

  centro <- t(a_vec)%*%betahat
  delta <- cuantil * sqrt(t(a_vec) %*% Sigmahat %*% a_vec)
  return(c(centro - delta, centro + delta))
}

cubre <- function(intervalo, valor) { intervalo[1] <= valor & intervalo[2] >= valor}

ayudante_intervalo_conf <- function(n_simulacion, distr_epsilon, n, fun_a, met_int, alfa) {
  muestra_a_evaluar <- (muestras_maestras %>% filter(n_sim==n_simulacion,distr_eps==distr_epsilon))[[1,'muestra']] %>% head(n)
  modelo <- lm(y ~ x1 + x2 + x3 +x4,data=muestra_a_evaluar)
  intervalo_conf(a_vec = funciones_a[[fun_a]], llamada_lm=modelo, alfa=alfa, metodo = met_int)
}

intervalos <- muestras_puntuales %>%
  crossing(
    fun_a = names(funciones_a),
    met_int = metodos_intervalo) %>%
  mutate(
    #atbeta es el valor del parámetro en el PGD.
    atbeta = map_dbl(fun_a, function(i) funciones_a[[i]] %*% beta_pgd),
    ic = future_pmap( .progress = TRUE,
      list(n_sim, distr_eps, n, fun_a, met_int),
      ayudante_intervalo_conf,
      alfa = alfa),
    cubre = map2_lgl(ic, atbeta, cubre),
    ic_low = map_dbl(ic, 1),
    ic_upp = map_dbl(ic, 2)
    )
```



# Respuestas

```{r tabla grande}

intervalos <- read_rds("simulacion.Rds") %>%
  mutate(
    estimador = (ic_upp+ic_low)/2
  )
```

## ¿Los intervalos Cubren a los Parámetros?

Respondemos directamente para todas las distribuciones estudiadas.

### Para $B=1$ y, es decir si el primer intento de obtener el intervalo cubre el valor.

```{r cubren  b1}
sintesis <- intervalos %>%
  filter(n_sim==1) %>%
  group_by(distr_eps, n, met_int, fun_a) %>%
  summarise(prop_cubre = mean(cubre))

sintesis %>%
  mutate (
    prop_cubre = round(digits=4,x=prop_cubre),
    prop_cubre = cell_spec(prop_cubre,"latex",
                           color = ifelse(prop_cubre < 0.89,"red","blue"),
                           background = ifelse(prop_cubre < 0.86,"gray","white")
                           )
  ) %>%
  spread(n,prop_cubre) %>%
  kable(format="latex", escape = F ) %>%
  kable_styling("condensed", "striped") %>%
  add_header_above(c(" "=3,"valores de n"=9))
```


### Para $B=1000$ 


```{r cubren  b1000}
sintesis <- intervalos %>%
  filter(n_sim<=1000) %>%
  group_by(distr_eps, n, met_int, fun_a) %>%
  summarise(prop_cubre = mean(cubre))

sintesis %>%
  mutate (
    prop_cubre = round(digits=4,x=prop_cubre),
    prop_cubre = cell_spec(prop_cubre,"latex",
                           color = ifelse(prop_cubre < 0.89,"red","blue"),
                           background = ifelse(prop_cubre < 0.86,"gray","white")
                           )
  ) %>%
  spread(n,prop_cubre) %>%
  kable(format="latex", escape = F) %>%
  kable_styling("condensed", "striped") %>%
  add_header_above(c(" "=3,"valores de n"=9))
```

### Para $B=3000$

```{r cubren  b3000}
sintesis <- intervalos %>%
  group_by(distr_eps, n, met_int, fun_a) %>%
  summarise(prop_cubre = mean(cubre))

sintesis %>%
  mutate (
    prop_cubre = round(digits=4,x=prop_cubre),
    prop_cubre = cell_spec(prop_cubre,"latex",
                           color = ifelse(prop_cubre < 0.89,"red","blue"),
                           background = ifelse(prop_cubre < 0.86,"gray","white")
                           )
  ) %>%
  spread(n,prop_cubre) %>%
  kable(format="latex", escape = F) %>%
  kable_styling("condensed", "striped") %>%
  add_header_above(c(" "=3,"valores de n"=9))
```

Observamos que para muestras pequeñas, con $n=10$, es usual que los intervalos asintóticos no lleguen a cubrir los parámetros. En varios casos incluido el exponencial nuestra media de aciertos está por debajo del $\alpha$ establecido.
Esto no mejora incrementando las repeticiones hasta 3.000, es decir que probablemente ya estábamos cerca de su límite.

### Proporción de Intervalos que no cubren el parámetro del PGD 

En este gráfico mostramos en columnas paralelas la proporción de casos donde no se cubrió el parámetro por método y por tamaño de muestra para cada variable considerada. 

#### Para el Error Exponencial


```{r intervalos exponenciales que cubren}
graficar_proporciones_cobertura <- function(distr_eps){
  intervalos %>%
    filter(distr_eps==distr_eps,cubre==FALSE, n_sim <= 1000) %>%
    ggplot(aes(x = met_int, y=..count../sum(..count..) , fill=n, color = met_int)) +
    geom_bar() +
    theme(
      axis.text.x.bottom = element_blank()
    ) +
    labs(
      x = element_blank(),
      y = "Proporción de fallas",
      color = "Método del Intervalo",
      fill = "n de la muestra"
    ) +
    facet_grid(fun_a~n)
}

graficar_proporciones_cobertura2 <- function(distr_eps){
  sintesis %>%
    filter(distr_eps==distr_eps) %>%
    ggplot(aes(x = n, y= prop_cubre , fill=n, color = met_int)) +
    geom_smooth() +
    theme(
      axis.text.x.bottom = element_blank()
    ) +
    facet_grid(.~met_int)
}

#graficar_proporciones_cobertura2('exponencial')

graficar_proporciones_cobertura('exponencial')
```

####  Para el Error Lognormal

```{r intervalos lognormales que cubren}
graficar_proporciones_cobertura('lognormal')
```

####  Para el Error Uniforme

```{r intervalos uniformes que cubren}
graficar_proporciones_cobertura('uniforme')
```

####  Para el Error Chi Cuadrado

```{r intervalos chi que cubren}
graficar_proporciones_cobertura('chi_cuadrado')
```

####  Para el Error T 

```{r intervalos T que cubren}
graficar_proporciones_cobertura('student3')
```

####  Para el error Cauchy

```{r intervalos cauchy que cubren}
graficar_proporciones_cobertura('student1')
```

### Algunas Observaciones Sobre Estos Patrones

  Se ven varios fenómenos que aprecen dignos de interés pero no querríamos interpretar de forma trivial.
  
  Los incrementos tanto en el tamaño de muestra como en el número de repeticiones brindan en principio un incremento en el apego al valor de $\alpha$ para los intervalos asintóticos. Después de una cantidad de valores vemos que ambos se comportan en forma prácticamente indistinguible aunque es notable la precisión inicial del intervalo exacto. 
  
  Es imposible incrementar nuestro apego en la tasa de acierto al alfa más allá de cierto nivel, de hecho en algunos casos observamos regresiones al incrementar el valor de $B$. Estas regresiones son pequeñas, en la mayoría de los casos una vez que nos acercamos lo suficiente en la proporción de aciertos a $1-\alpha$ permanecemos ahí, pero en el gráfico se ven exageradas por la escala. Originalmente habíamos hecho un gráfico de tasa de aciertos en vez de tasa de errores y eran bastante difíciles de percibir.  
  
  En el caso de la distribución de Cauchy es tentador pensar que si bien no hay esperanza finita, hay una región que concentra la máxima densidad y por tanto define un intervalo de máxima probabilidad. La ausencia de esperanza es porque las colas no convergen a 0 con suficiente "rapidez" y eso nos va a afectar a la hora de estudiar cómo se portan los estimadores cuando el error tiene esta forma. El intervalo, en cambio, funciona razonablemente bien. 

## ¿Los Estimadores Tienen Distribución Normal?

### Normalidad de $\hat{\beta}_4$ cuando el error es exponencial.

El siguiente gráfico nos muestra el gráfico QQ para el parámetro $\hat{\beta}_4$ y su evolución al incrementar la cantidad de muestras $n$.

El mismo tiene una escala fija para que tenga sentido la comparación gráfica.

```{r qq exponencial}
intervalos %>%
  filter(n_sim <= 50, is.element(n,c(10,100,1000,3000)),fun_a=='beta4',distr_eps == 'exponencial')  %>%
  ggplot() +
  aes(sample =estimador, color = distr_eps) +
  geom_qq() +
  stat_qq_line() +
  facet_grid(n~.) +
  labs(
    x = "Muestra",
    y = "Teórico"
  ) +
  guides( color = FALSE )


```

  Es interesante observar qué pasa si vamos reduciendo la escala también: El patrón de distancia a la recta de los cuantiles ideales se mantiene similar, aunque estas se ven reducidas hasta tal punto que en el otro gráfico donde mantenemos la escala constante parecen adherirse a ella.
  Pareciera observarse el efecto aritmético de dividir por $n$, manteniéndose la relación de aspecto entre las distancias.

```{r qq exponencial libre}
intervalos %>%
  filter(n_sim <= 50, is.element(n,c(10,100,1000,3000)),fun_a=='beta4',distr_eps == 'exponencial')  %>%
  ggplot() +
  aes(sample =estimador, color = distr_eps) +
  geom_qq() +
  stat_qq_line() +
  facet_grid(n~., scales="free") +
  labs(
    x = "Muestra",
    y = "Teórico"
  ) +
  guides( color = FALSE )

```

### Normalidad de $\hat{\beta}_4$ para los diversos errores según $n$.

```{r qqses por n y distr_eps}
intervalos %>%
  filter(n_sim <= 50, is.element(n,c(10,100,1000,3000)),fun_a=='beta1')  %>%
  ggplot() +
  aes(sample =estimador, color = distr_eps) +
  geom_qq() +
  stat_qq_line() +
  facet_grid(n~distr_eps) +
  labs(
    x = "Muestra",
    y = "Teórico"
  ) +
  guides( color = FALSE )
```

#### Conclusiones sobre la Normalidad del Estimador

  Como vemos, incrementando lo suficiente el valor de $n$ esta se termina manifestando rápidamente, aún cuando para el gráfico dejamos fijo el valor $B=50$, uno muy chico respecto a los totales que manejamos, de hasta 3.000. 
  Por supuesto, este fenómeno no se presenta sólo en las distribuciones que cumplen las hipótesis del Teorema del Límite central. En ese sentido la inclusión de error con distribución $t_1$ nos permitió ver qué pasa cuando no es correcta la aproximación normal. En todos los casos observamos puntos que están drásticamente lejos de los cuantiles de la distribución normal.  
 
# Conclusiones

  En general se observan varias tendencias. 
  
* Para todas las distribuciones obtuvimos mejores resultados con los intervalos exactos. 
* En casi todas al incrementar $n$ logramos llevar el nivel de los intervalos asintóticos prácticamente al de los exactos, a partir de ciertos tamaños se mueven a la par, es decir que estiman casi idénticamente. 
* La única distribución del error que hizo que el estimador se comporte fuera de lo esperable fue la Cauchy, todas las pedidas por la consigna terminan por converger a un comportamiento asintóticamente normal. 
* Sería interesante buscar conjuntos de datos que no vengan de distribuciones teóricas para ver si en ese caso los intervalos asintóticos se comportan mejor. 
* Sería interesante hacer un estudio que contemple también la anchura de los intervalos. 
